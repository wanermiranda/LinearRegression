{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from diamonds import experiments, normal_equation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "scoring = {\n",
    "    \t'Negative MSE': 'neg_mean_squared_error',\n",
    "    \t'Negative MAE': 'neg_mean_absolute_error',\n",
    "    \t'R2': 'r2'\n",
    "\t}\n",
    "\n",
    "val_size = .15\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discution 1\n",
    "  - Using the SKlearn SGDregressor with basic params we are now comparing the results.\n",
    "  - Kept only the best results from the first experiment to run the GridSearch for the parameters\n",
    "  - The Log(Y) kept the algorithm more robust reducing the errors mean value\n",
    "  - The Scale kept the algorithm more robust reducing the errors mean value\n",
    "  - The syntetic features wherever they appeards reduces the standard deviation from the MAE/MSE and RSME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = experiments.get_sklearn_sgd(params)\n",
    "regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=['ratio_xz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1609.3210 +/- 97.7450\n",
      "MSE:  \t 2599468.0829 +/- 319313.5273\n",
      "MAE:  \t 714.2778 +/- 27.7334\n",
      "R2:   \t 0.8172 +/- 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=['ratio_xy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1633.7756 +/- 91.1391\n",
      "MSE:  \t 2677528.9644 +/- 302051.6732\n",
      "MAE:  \t 718.6580 +/- 28.2871\n",
      "R2:   \t 0.8117 +/- 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xy', 'ratio_xz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1541.6432 +/- 91.7475\n",
      "MSE:  \t 2385081.3106 +/- 277095.3156\n",
      "MAE:  \t 697.8126 +/- 20.5680\n",
      "R2:   \t 0.8321 +/- 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discution 2\n",
    "  - The results doesn't appear to have an statistical difference between then, since the mean and std deviation are practically the same. Although the result with features included has a better R2. \n",
    "  - The SGD goes almost to the same minimal as the normal equation results.\n",
    "  - Whe are now running the GridSearch CV for the SGD to look for better parameters and will be using the last dataset above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_iter': [5000], 'eta0': [0.1, 0.05, 0.01], 'loss': ['squared_loss'], 'penalty': ['l2', 'l1', None], 'learning_rate': ['invscaling', 'optimal', 'constant']},\n",
       "       pre_dispatch='2*n_jobs', refit='R2', return_train_score='warn',\n",
       "       scoring={'-MSE': 'neg_mean_squared_error', '-MAE': 'neg_mean_absolute_error', 'R2': 'r2'},\n",
       "       verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'learning_rate':['invscaling', 'optimal', 'constant'],\n",
    "    'eta0': [0.1, 0.05, 0.01], # since 0.01 had a good result in the previous results \n",
    "    'penalty': ['l2', 'l1', None], # Those penalties are easier to implement if needed\n",
    "    'loss': ['squared_loss'], # Since we are running the MSE loss function for the Custom Implementing\n",
    "    'max_iter':[5000] # Fixed the number of iterations to avoid the long time executions\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "        '-MSE': 'neg_mean_squared_error',\n",
    "        '-MAE': 'neg_mean_absolute_error',\n",
    "        'R2': 'r2'\n",
    "    }\n",
    "\n",
    "# We are using R2 to refit because it gave a better view of the results above when compared with the MSE and MAE\n",
    "regr = GridSearchCV(regr, params, cv=5, scoring=scoring, refit='R2', n_jobs=-1, verbose=True)\n",
    "regr.fit(X_train, np.log(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.1,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=5000, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta0': 0.1,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 5000,\n",
       " 'penalty': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['std_train_-MAE', 'split1_train_-MSE', 'std_fit_time', 'split2_train_-MSE', 'mean_train_-MAE', 'split0_train_R2', 'mean_test_R2', 'split3_test_R2', 'split0_train_-MSE', 'mean_train_R2', 'std_train_-MSE', 'split4_train_-MAE', 'param_learning_rate', 'split2_test_-MSE', 'rank_test_-MSE', 'mean_fit_time', 'split2_test_-MAE', 'split1_test_-MSE', 'std_train_R2', 'split1_train_-MAE', 'split2_train_-MAE', 'std_test_R2', 'split3_test_-MSE', 'split1_test_-MAE', 'rank_test_-MAE', 'std_test_-MSE', 'split3_train_R2', 'split3_train_-MSE', 'split4_test_-MAE', 'param_max_iter', 'split4_test_-MSE', 'std_test_-MAE', 'param_loss', 'split4_train_-MSE', 'params', 'split0_test_-MSE', 'split4_test_R2', 'rank_test_R2', 'mean_test_-MSE', 'split0_test_R2', 'split2_train_R2', 'param_eta0', 'mean_train_-MSE', 'mean_score_time', 'split3_train_-MAE', 'split0_test_-MAE', 'split4_train_R2', 'split3_test_-MAE', 'std_score_time', 'split0_train_-MAE', 'param_penalty', 'mean_test_-MAE', 'split1_test_R2', 'split2_test_R2', 'split1_train_R2'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>-0.013529</td>\n",
       "      <td>-0.091500</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>21.696494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-0.013669</td>\n",
       "      <td>-0.092389</td>\n",
       "      <td>0.985956</td>\n",
       "      <td>10.615460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>-0.013952</td>\n",
       "      <td>-0.092925</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>20.377451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>-0.096400</td>\n",
       "      <td>0.984586</td>\n",
       "      <td>20.512749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.096519</td>\n",
       "      <td>0.984560</td>\n",
       "      <td>26.909482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>-0.015047</td>\n",
       "      <td>-0.096410</td>\n",
       "      <td>0.984540</td>\n",
       "      <td>28.861118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>-0.015105</td>\n",
       "      <td>-0.096752</td>\n",
       "      <td>0.984481</td>\n",
       "      <td>21.109486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.015122</td>\n",
       "      <td>-0.096711</td>\n",
       "      <td>0.984464</td>\n",
       "      <td>21.883833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>-0.015194</td>\n",
       "      <td>-0.097098</td>\n",
       "      <td>0.984389</td>\n",
       "      <td>26.829921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>-0.015237</td>\n",
       "      <td>-0.097252</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>20.958378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling           5000  squared_loss        0.1   \n",
       "2                       constant           5000  squared_loss       0.01   \n",
       "3                     invscaling           5000  squared_loss       0.05   \n",
       "4                     invscaling           5000  squared_loss       0.01   \n",
       "5                     invscaling           5000  squared_loss       0.05   \n",
       "6                     invscaling           5000  squared_loss        0.1   \n",
       "7                     invscaling           5000  squared_loss       0.05   \n",
       "8                     invscaling           5000  squared_loss        0.1   \n",
       "9                     invscaling           5000  squared_loss       0.01   \n",
       "10                    invscaling           5000  squared_loss       0.01   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000506   \n",
       "2                     None               2               2       0.000467   \n",
       "3                     None               3               3       0.000504   \n",
       "4                     None               4               4       0.000534   \n",
       "5                       l1               5               6       0.000503   \n",
       "6                       l1               6               5       0.000487   \n",
       "7                       l2               7               8       0.000511   \n",
       "8                       l2               8               7       0.000494   \n",
       "9                       l1               9               9       0.000519   \n",
       "10                      l2              10              10       0.000520   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001492     0.000575       -0.013529       -0.091500   \n",
       "2                  0.001516     0.000532       -0.013669       -0.092389   \n",
       "3                  0.001400     0.000573       -0.013952       -0.092925   \n",
       "4                  0.001483     0.000609       -0.015003       -0.096400   \n",
       "5                  0.001425     0.000581       -0.015028       -0.096519   \n",
       "6                  0.001322     0.000559       -0.015047       -0.096410   \n",
       "7                  0.001443     0.000586       -0.015105       -0.096752   \n",
       "8                  0.001396     0.000561       -0.015122       -0.096711   \n",
       "9                  0.001487     0.000596       -0.015194       -0.097098   \n",
       "10                 0.001477     0.000597       -0.015237       -0.097252   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986100      21.696494  \n",
       "2                 0.985956      10.615460  \n",
       "3                 0.985665      20.377451  \n",
       "4                 0.984586      20.512749  \n",
       "5                 0.984560      26.909482  \n",
       "6                 0.984540      28.861118  \n",
       "7                 0.984481      21.109486  \n",
       "8                 0.984464      21.883833  \n",
       "9                 0.984389      26.829921  \n",
       "10                0.984345      20.958378  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "columns = [\n",
    "'param_learning_rate',\n",
    "'param_max_iter',\n",
    "'param_loss',\n",
    "'param_eta0',\n",
    "'param_penalty',\n",
    "'rank_test_-MSE',\n",
    "'rank_test_-MAE',\n",
    "'rank_test_R2',\n",
    "'std_test_-MSE',\n",
    "'std_test_-MAE',\n",
    "'std_test_R2',\n",
    "'mean_test_-MSE',\n",
    "'mean_test_-MAE',\n",
    "'mean_test_R2', \n",
    "'mean_fit_time']\n",
    "\n",
    "results = pd.DataFrame(regr.cv_results_)\n",
    "top10 = results[columns].sort_values(by=['rank_test_R2', 'mean_test_R2']).head(10).copy()\n",
    "top10.sort_values(by=['rank_test_R2', 'mean_test_R2'])\n",
    "top10.set_index('rank_test_R2', inplace=True, drop=True)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f2ffd9a20>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10.mean_fit_time.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f2ff5aa58>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "top10.mean_test_R2.plot.bar(yerr=top10.std_test_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f2feb3198>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top10.std_test_R2.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>-0.013529</td>\n",
       "      <td>-0.091500</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>21.696494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-0.013669</td>\n",
       "      <td>-0.092389</td>\n",
       "      <td>0.985956</td>\n",
       "      <td>10.615460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>-0.013952</td>\n",
       "      <td>-0.092925</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>20.377451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>-0.096400</td>\n",
       "      <td>0.984586</td>\n",
       "      <td>20.512749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.096519</td>\n",
       "      <td>0.984560</td>\n",
       "      <td>26.909482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>-0.015047</td>\n",
       "      <td>-0.096410</td>\n",
       "      <td>0.984540</td>\n",
       "      <td>28.861118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>-0.015105</td>\n",
       "      <td>-0.096752</td>\n",
       "      <td>0.984481</td>\n",
       "      <td>21.109486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.015122</td>\n",
       "      <td>-0.096711</td>\n",
       "      <td>0.984464</td>\n",
       "      <td>21.883833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>-0.015194</td>\n",
       "      <td>-0.097098</td>\n",
       "      <td>0.984389</td>\n",
       "      <td>26.829921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>-0.015237</td>\n",
       "      <td>-0.097252</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>20.958378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling           5000  squared_loss        0.1   \n",
       "2                       constant           5000  squared_loss       0.01   \n",
       "3                     invscaling           5000  squared_loss       0.05   \n",
       "4                     invscaling           5000  squared_loss       0.01   \n",
       "5                     invscaling           5000  squared_loss       0.05   \n",
       "6                     invscaling           5000  squared_loss        0.1   \n",
       "7                     invscaling           5000  squared_loss       0.05   \n",
       "8                     invscaling           5000  squared_loss        0.1   \n",
       "9                     invscaling           5000  squared_loss       0.01   \n",
       "10                    invscaling           5000  squared_loss       0.01   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000506   \n",
       "2                     None               2               2       0.000467   \n",
       "3                     None               3               3       0.000504   \n",
       "4                     None               4               4       0.000534   \n",
       "5                       l1               5               6       0.000503   \n",
       "6                       l1               6               5       0.000487   \n",
       "7                       l2               7               8       0.000511   \n",
       "8                       l2               8               7       0.000494   \n",
       "9                       l1               9               9       0.000519   \n",
       "10                      l2              10              10       0.000520   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001492     0.000575       -0.013529       -0.091500   \n",
       "2                  0.001516     0.000532       -0.013669       -0.092389   \n",
       "3                  0.001400     0.000573       -0.013952       -0.092925   \n",
       "4                  0.001483     0.000609       -0.015003       -0.096400   \n",
       "5                  0.001425     0.000581       -0.015028       -0.096519   \n",
       "6                  0.001322     0.000559       -0.015047       -0.096410   \n",
       "7                  0.001443     0.000586       -0.015105       -0.096752   \n",
       "8                  0.001396     0.000561       -0.015122       -0.096711   \n",
       "9                  0.001487     0.000596       -0.015194       -0.097098   \n",
       "10                 0.001477     0.000597       -0.015237       -0.097252   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986100      21.696494  \n",
       "2                 0.985956      10.615460  \n",
       "3                 0.985665      20.377451  \n",
       "4                 0.984586      20.512749  \n",
       "5                 0.984560      26.909482  \n",
       "6                 0.984540      28.861118  \n",
       "7                 0.984481      21.109486  \n",
       "8                 0.984464      21.883833  \n",
       "9                 0.984389      26.829921  \n",
       "10                0.984345      20.958378  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the search\n",
    "   - Given the above rank we are selecting the 3 top configurations and re-running the grid with more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 20.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_iter': [100000], 'eta0': [0.1, 0.05, 0.01], 'loss': ['squared_loss'], 'penalty': [None], 'learning_rate': ['invscaling', 'constant']},\n",
       "       pre_dispatch='2*n_jobs', refit='R2', return_train_score='warn',\n",
       "       scoring={'-MSE': 'neg_mean_squared_error', '-MAE': 'neg_mean_absolute_error', 'R2': 'r2'},\n",
       "       verbose=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "regr = experiments.get_sklearn_sgd(params)\n",
    "regr.verbose = False\n",
    "params = {\n",
    "    'learning_rate':['invscaling','constant'],\n",
    "    'eta0': [0.1, 0.05, 0.01], # since 0.01 had a good result in the previous results \n",
    "    'penalty': [None], # Those penalties are easier to implement if needed\n",
    "    'loss': ['squared_loss'], # Since we are running the MSE loss function for the Custom Implementing\n",
    "    'max_iter':[100000] # Fixed the number of iterations to avoid the long time executions\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "        '-MSE': 'neg_mean_squared_error',\n",
    "        '-MAE': 'neg_mean_absolute_error',\n",
    "        'R2': 'r2'\n",
    "    }\n",
    "\n",
    "# We are using R2 to refit because it gave a better view of the results above when compared with the MSE and MAE\n",
    "regr = GridSearchCV(regr, params, cv=5, scoring=scoring, refit='R2', n_jobs=-1, verbose=True)\n",
    "regr.fit(X_train, np.log(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta0': 0.05,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 500000,\n",
       " 'penalty': None}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
