{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from diamonds import experiments, normal_equation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "scoring = {\n",
    "    \t'Negative MSE': 'neg_mean_squared_error',\n",
    "    \t'Negative MAE': 'neg_mean_absolute_error',\n",
    "    \t'R2': 'r2'\n",
    "\t}\n",
    "\n",
    "val_size = .15\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discution 1\n",
    "  - Using the SKlearn SGDregressor with basic params we are now comparing the results.\n",
    "  - Kept only the best results from the first experiment to run the GridSearch for the parameters\n",
    "  - The Log(Y) kept the algorithm more robust reducing the errors mean value\n",
    "  - The Scale kept the algorithm more robust reducing the errors mean value\n",
    "  - The syntetic features wherever they appeards reduces the standard deviation from the MAE/MSE and RSME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = experiments.get_sklearn_sgd(params)\n",
    "regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=['ratio_xz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1623.5299 +/- 98.0607\n",
      "MSE:  \t 2645465.1322 +/- 314672.4479\n",
      "MAE:  \t 719.4283 +/- 23.8322\n",
      "R2:   \t 0.8138 +/- 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=['ratio_xy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1588.3106 +/- 101.2143\n",
      "MSE:  \t 2532974.9834 +/- 336302.1876\n",
      "MAE:  \t 705.9682 +/- 29.9954\n",
      "R2:   \t 0.8218 +/- 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding syntect features ['volume', 'ratio_xy', 'ratio_xz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiments.load_train_data()\n",
    "folds, (X_train, X_test, y_train, y_test) = experiments.gen_splits(X, scale=True, \n",
    "                                                             exclude_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0\n",
      "Evaluating 1\n",
      "Evaluating 2\n",
      "Evaluating 3\n",
      "Evaluating 4\n",
      "RMSE: \t 1589.3804 +/- 111.7284\n",
      "MSE:  \t 2538613.2083 +/- 366494.7774\n",
      "MAE:  \t 710.8763 +/- 31.5879\n",
      "R2:   \t 0.8214 +/- 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "experiments.kfold_evaluate(regr, folds, scoring, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discution 2\n",
    "  - The results doesn't appear to have an statistical difference between then, since the mean and std deviation are practically the same. Although the result with features included has a better R2. \n",
    "  - The SGD goes almost to the same minimal as the normal equation results.\n",
    "  - Whe are now running the GridSearch CV for the SGD to look for better parameters and will be using the last dataset above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l2', 'l1', None], 'loss': ['squared_loss'], 'max_iter': [5000], 'learning_rate': ['invscaling', 'optimal', 'constant'], 'eta0': [0.1, 0.05, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit='R2', return_train_score='warn',\n",
       "       scoring={'-MAE': 'neg_mean_absolute_error', 'R2': 'r2', '-MSE': 'neg_mean_squared_error'},\n",
       "       verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'learning_rate':['invscaling', 'optimal', 'constant'],\n",
    "    'eta0': [0.1, 0.05, 0.01], # since 0.01 had a good result in the previous results \n",
    "    'penalty': ['l2', 'l1', None], # Those penalties are easier to implement if needed\n",
    "    'loss': ['squared_loss'], # Since we are running the MSE loss function for the Custom Implementing\n",
    "    'max_iter':[5000] # Fixed the number of iterations to avoid the long time executions\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "        '-MSE': 'neg_mean_squared_error',\n",
    "        '-MAE': 'neg_mean_absolute_error',\n",
    "        'R2': 'r2'\n",
    "    }\n",
    "\n",
    "# We are using R2 to refit because it gave a better view of the results above when compared with the MSE and MAE\n",
    "regr = GridSearchCV(regr, params, cv=5, scoring=scoring, refit='R2', n_jobs=-1, verbose=True)\n",
    "regr.fit(X_train, np.log(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "       loss='squared_loss', max_iter=5000, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta0': 0.01,\n",
       " 'learning_rate': 'constant',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 5000,\n",
       " 'penalty': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['param_eta0', 'split0_train_-MSE', 'split4_train_-MAE', 'mean_train_-MSE', 'split2_test_-MAE', 'rank_test_-MSE', 'split0_test_-MSE', 'std_train_R2', 'mean_test_-MSE', 'std_score_time', 'param_loss', 'split2_test_R2', 'param_penalty', 'rank_test_R2', 'split3_train_-MAE', 'split4_train_R2', 'split2_train_R2', 'split3_test_-MAE', 'mean_test_-MAE', 'param_learning_rate', 'split4_test_-MAE', 'std_test_-MSE', 'split0_test_-MAE', 'split2_train_-MSE', 'split1_test_R2', 'split1_test_-MAE', 'mean_test_R2', 'split2_train_-MAE', 'split2_test_-MSE', 'rank_test_-MAE', 'split0_test_R2', 'params', 'std_test_R2', 'mean_train_-MAE', 'std_train_-MSE', 'split1_train_R2', 'mean_train_R2', 'param_max_iter', 'split3_train_-MSE', 'split3_train_R2', 'std_test_-MAE', 'mean_score_time', 'split3_test_R2', 'std_train_-MAE', 'split0_train_R2', 'split4_test_-MSE', 'mean_fit_time', 'std_fit_time', 'split1_train_-MAE', 'split3_test_-MSE', 'split1_test_-MSE', 'split1_train_-MSE', 'split4_train_-MSE', 'split4_test_R2', 'split0_train_-MAE'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.013497</td>\n",
       "      <td>-0.091654</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>10.943297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>-0.013540</td>\n",
       "      <td>-0.091769</td>\n",
       "      <td>0.986089</td>\n",
       "      <td>23.961097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>-0.013928</td>\n",
       "      <td>-0.092772</td>\n",
       "      <td>0.985690</td>\n",
       "      <td>21.347103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.014965</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>0.984625</td>\n",
       "      <td>12.581060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>-0.015004</td>\n",
       "      <td>-0.096484</td>\n",
       "      <td>0.984585</td>\n",
       "      <td>20.943546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>-0.015027</td>\n",
       "      <td>-0.096597</td>\n",
       "      <td>0.984561</td>\n",
       "      <td>31.900651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-0.015033</td>\n",
       "      <td>-0.096383</td>\n",
       "      <td>0.984555</td>\n",
       "      <td>28.259650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.015085</td>\n",
       "      <td>-0.096858</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>22.078471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>-0.015115</td>\n",
       "      <td>-0.096960</td>\n",
       "      <td>0.984470</td>\n",
       "      <td>23.298074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.015202</td>\n",
       "      <td>-0.097156</td>\n",
       "      <td>0.984381</td>\n",
       "      <td>26.597084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                       constant           5000  squared_loss       0.01   \n",
       "2                     invscaling           5000  squared_loss        0.1   \n",
       "3                     invscaling           5000  squared_loss       0.05   \n",
       "4                       constant           5000  squared_loss       0.05   \n",
       "5                     invscaling           5000  squared_loss       0.01   \n",
       "6                     invscaling           5000  squared_loss        0.1   \n",
       "7                     invscaling           5000  squared_loss       0.05   \n",
       "8                     invscaling           5000  squared_loss       0.05   \n",
       "9                     invscaling           5000  squared_loss        0.1   \n",
       "10                    invscaling           5000  squared_loss       0.01   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000448   \n",
       "2                     None               2               2       0.000512   \n",
       "3                     None               3               3       0.000504   \n",
       "4                     None               4               8       0.000703   \n",
       "5                     None               5               5       0.000519   \n",
       "6                       l1               6               6       0.000538   \n",
       "7                       l1               7               4       0.000531   \n",
       "8                       l2               8               7       0.000515   \n",
       "9                       l2               9               9       0.000541   \n",
       "10                      l1              10              10       0.000512   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001559     0.000500       -0.013497       -0.091654   \n",
       "2                  0.001595     0.000577       -0.013540       -0.091769   \n",
       "3                  0.001452     0.000572       -0.013928       -0.092772   \n",
       "4                  0.002083     0.000771       -0.014965       -0.096874   \n",
       "5                  0.001465     0.000594       -0.015004       -0.096484   \n",
       "6                  0.001570     0.000615       -0.015027       -0.096597   \n",
       "7                  0.001396     0.000603       -0.015033       -0.096383   \n",
       "8                  0.001427     0.000589       -0.015085       -0.096858   \n",
       "9                  0.001595     0.000621       -0.015115       -0.096960   \n",
       "10                 0.001475     0.000591       -0.015202       -0.097156   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986134      10.943297  \n",
       "2                 0.986089      23.961097  \n",
       "3                 0.985690      21.347103  \n",
       "4                 0.984625      12.581060  \n",
       "5                 0.984585      20.943546  \n",
       "6                 0.984561      31.900651  \n",
       "7                 0.984555      28.259650  \n",
       "8                 0.984501      22.078471  \n",
       "9                 0.984470      23.298074  \n",
       "10                0.984381      26.597084  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "columns = [\n",
    "'param_learning_rate',\n",
    "'param_max_iter',\n",
    "'param_loss',\n",
    "'param_eta0',\n",
    "'param_penalty',\n",
    "'rank_test_-MSE',\n",
    "'rank_test_-MAE',\n",
    "'rank_test_R2',\n",
    "'std_test_-MSE',\n",
    "'std_test_-MAE',\n",
    "'std_test_R2',\n",
    "'mean_test_-MSE',\n",
    "'mean_test_-MAE',\n",
    "'mean_test_R2', \n",
    "'mean_fit_time']\n",
    "\n",
    "results = pd.DataFrame(regr.cv_results_)\n",
    "top10 = results[columns].sort_values(by=['rank_test_R2', 'mean_test_R2']).head(10).copy()\n",
    "top10.sort_values(by=['rank_test_R2', 'mean_test_R2'])\n",
    "top10.set_index('rank_test_R2', inplace=True, drop=True)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8fb1fa19b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10.mean_fit_time.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8faf27b8d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "top10.mean_test_R2.plot.bar(yerr=top10.std_test_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8fb1821898>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top10.std_test_R2.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.013497</td>\n",
       "      <td>-0.091654</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>10.943297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>-0.013540</td>\n",
       "      <td>-0.091769</td>\n",
       "      <td>0.986089</td>\n",
       "      <td>23.961097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>-0.013928</td>\n",
       "      <td>-0.092772</td>\n",
       "      <td>0.985690</td>\n",
       "      <td>21.347103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.014965</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>0.984625</td>\n",
       "      <td>12.581060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>-0.015004</td>\n",
       "      <td>-0.096484</td>\n",
       "      <td>0.984585</td>\n",
       "      <td>20.943546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>-0.015027</td>\n",
       "      <td>-0.096597</td>\n",
       "      <td>0.984561</td>\n",
       "      <td>31.900651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-0.015033</td>\n",
       "      <td>-0.096383</td>\n",
       "      <td>0.984555</td>\n",
       "      <td>28.259650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>l2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.015085</td>\n",
       "      <td>-0.096858</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>22.078471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>-0.015115</td>\n",
       "      <td>-0.096960</td>\n",
       "      <td>0.984470</td>\n",
       "      <td>23.298074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>5000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.015202</td>\n",
       "      <td>-0.097156</td>\n",
       "      <td>0.984381</td>\n",
       "      <td>26.597084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                       constant           5000  squared_loss       0.01   \n",
       "2                     invscaling           5000  squared_loss        0.1   \n",
       "3                     invscaling           5000  squared_loss       0.05   \n",
       "4                       constant           5000  squared_loss       0.05   \n",
       "5                     invscaling           5000  squared_loss       0.01   \n",
       "6                     invscaling           5000  squared_loss        0.1   \n",
       "7                     invscaling           5000  squared_loss       0.05   \n",
       "8                     invscaling           5000  squared_loss       0.05   \n",
       "9                     invscaling           5000  squared_loss        0.1   \n",
       "10                    invscaling           5000  squared_loss       0.01   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000448   \n",
       "2                     None               2               2       0.000512   \n",
       "3                     None               3               3       0.000504   \n",
       "4                     None               4               8       0.000703   \n",
       "5                     None               5               5       0.000519   \n",
       "6                       l1               6               6       0.000538   \n",
       "7                       l1               7               4       0.000531   \n",
       "8                       l2               8               7       0.000515   \n",
       "9                       l2               9               9       0.000541   \n",
       "10                      l1              10              10       0.000512   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001559     0.000500       -0.013497       -0.091654   \n",
       "2                  0.001595     0.000577       -0.013540       -0.091769   \n",
       "3                  0.001452     0.000572       -0.013928       -0.092772   \n",
       "4                  0.002083     0.000771       -0.014965       -0.096874   \n",
       "5                  0.001465     0.000594       -0.015004       -0.096484   \n",
       "6                  0.001570     0.000615       -0.015027       -0.096597   \n",
       "7                  0.001396     0.000603       -0.015033       -0.096383   \n",
       "8                  0.001427     0.000589       -0.015085       -0.096858   \n",
       "9                  0.001595     0.000621       -0.015115       -0.096960   \n",
       "10                 0.001475     0.000591       -0.015202       -0.097156   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986134      10.943297  \n",
       "2                 0.986089      23.961097  \n",
       "3                 0.985690      21.347103  \n",
       "4                 0.984625      12.581060  \n",
       "5                 0.984585      20.943546  \n",
       "6                 0.984561      31.900651  \n",
       "7                 0.984555      28.259650  \n",
       "8                 0.984501      22.078471  \n",
       "9                 0.984470      23.298074  \n",
       "10                0.984381      26.597084  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the search\n",
    "   - Given the above rank we are selecting the 3 top configurations and re-running the grid with more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 21.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': [None], 'loss': ['squared_loss'], 'max_iter': [100000], 'learning_rate': ['invscaling', 'constant'], 'eta0': [0.1, 0.05, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit='R2', return_train_score='warn',\n",
       "       scoring={'-MAE': 'neg_mean_absolute_error', 'R2': 'r2', '-MSE': 'neg_mean_squared_error'},\n",
       "       verbose=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "regr = experiments.get_sklearn_sgd(params)\n",
    "regr.verbose = False\n",
    "params = {\n",
    "    'learning_rate':['invscaling','constant'],\n",
    "    'eta0': [0.1, 0.05, 0.01], # since 0.01 had a good result in the previous results \n",
    "    'penalty': [None], # Those penalties are easier to implement if needed\n",
    "    'loss': ['squared_loss'], # Since we are running the MSE loss function for the Custom Implementing\n",
    "    'max_iter':[100000] # Fixed the number of iterations to avoid the long time executions\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "        '-MSE': 'neg_mean_squared_error',\n",
    "        '-MAE': 'neg_mean_absolute_error',\n",
    "        'R2': 'r2'\n",
    "    }\n",
    "\n",
    "# We are using R2 to refit because it gave a better view of the results above when compared with the MSE and MAE\n",
    "regr = GridSearchCV(regr, params, cv=5, scoring=scoring, refit='R2', n_jobs=-1, verbose=True)\n",
    "regr.fit(X_train, np.log(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>-0.091036</td>\n",
       "      <td>0.986287</td>\n",
       "      <td>419.791378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-0.013357</td>\n",
       "      <td>-0.091042</td>\n",
       "      <td>0.986276</td>\n",
       "      <td>426.281453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.091617</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>397.558125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-0.013584</td>\n",
       "      <td>-0.091974</td>\n",
       "      <td>0.986044</td>\n",
       "      <td>209.449511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>-0.014770</td>\n",
       "      <td>-0.095160</td>\n",
       "      <td>0.984828</td>\n",
       "      <td>257.370240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.025165</td>\n",
       "      <td>-0.124897</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>260.817710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling         100000  squared_loss       0.05   \n",
       "2                     invscaling         100000  squared_loss        0.1   \n",
       "3                     invscaling         100000  squared_loss       0.01   \n",
       "4                       constant         100000  squared_loss       0.01   \n",
       "5                       constant         100000  squared_loss       0.05   \n",
       "6                       constant         100000  squared_loss        0.1   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000483   \n",
       "2                     None               2               2       0.000491   \n",
       "3                     None               3               3       0.000495   \n",
       "4                     None               4               4       0.000407   \n",
       "5                     None               5               5       0.001060   \n",
       "6                     None               6               6       0.006079   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001480     0.000550       -0.013347       -0.091036   \n",
       "2                  0.001504     0.000558       -0.013357       -0.091042   \n",
       "3                  0.001434     0.000563       -0.013536       -0.091617   \n",
       "4                  0.001359     0.000468       -0.013584       -0.091974   \n",
       "5                  0.003164     0.001077       -0.014770       -0.095160   \n",
       "6                  0.015979     0.006186       -0.025165       -0.124897   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986287     419.791378  \n",
       "2                 0.986276     426.281453  \n",
       "3                 0.986093     397.558125  \n",
       "4                 0.986044     209.449511  \n",
       "5                 0.984828     257.370240  \n",
       "6                 0.974170     260.817710  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "columns = [\n",
    "'param_learning_rate',\n",
    "'param_max_iter',\n",
    "'param_loss',\n",
    "'param_eta0',\n",
    "'param_penalty',\n",
    "'rank_test_-MSE',\n",
    "'rank_test_-MAE',\n",
    "'rank_test_R2',\n",
    "'std_test_-MSE',\n",
    "'std_test_-MAE',\n",
    "'std_test_R2',\n",
    "'mean_test_-MSE',\n",
    "'mean_test_-MAE',\n",
    "'mean_test_R2', \n",
    "'mean_fit_time']\n",
    "\n",
    "results = pd.DataFrame(regr.cv_results_)\n",
    "top10 = results[columns].sort_values(by=['rank_test_R2', 'mean_test_R2']).head(10).copy()\n",
    "top10.sort_values(by=['rank_test_R2', 'mean_test_R2'])\n",
    "top10.set_index('rank_test_R2', inplace=True, drop=True)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8fb17f68d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10.mean_fit_time.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8fb17bc518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "top10.mean_test_R2.plot.bar(yerr=top10.std_test_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8fd77a0978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top10.std_test_R2.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>-0.091036</td>\n",
       "      <td>0.986287</td>\n",
       "      <td>419.791378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-0.013357</td>\n",
       "      <td>-0.091042</td>\n",
       "      <td>0.986276</td>\n",
       "      <td>426.281453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.091617</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>397.558125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-0.013584</td>\n",
       "      <td>-0.091974</td>\n",
       "      <td>0.986044</td>\n",
       "      <td>209.449511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>-0.014770</td>\n",
       "      <td>-0.095160</td>\n",
       "      <td>0.984828</td>\n",
       "      <td>257.370240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.025165</td>\n",
       "      <td>-0.124897</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>260.817710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling         100000  squared_loss       0.05   \n",
       "2                     invscaling         100000  squared_loss        0.1   \n",
       "3                     invscaling         100000  squared_loss       0.01   \n",
       "4                       constant         100000  squared_loss       0.01   \n",
       "5                       constant         100000  squared_loss       0.05   \n",
       "6                       constant         100000  squared_loss        0.1   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000483   \n",
       "2                     None               2               2       0.000491   \n",
       "3                     None               3               3       0.000495   \n",
       "4                     None               4               4       0.000407   \n",
       "5                     None               5               5       0.001060   \n",
       "6                     None               6               6       0.006079   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001480     0.000550       -0.013347       -0.091036   \n",
       "2                  0.001504     0.000558       -0.013357       -0.091042   \n",
       "3                  0.001434     0.000563       -0.013536       -0.091617   \n",
       "4                  0.001359     0.000468       -0.013584       -0.091974   \n",
       "5                  0.003164     0.001077       -0.014770       -0.095160   \n",
       "6                  0.015979     0.006186       -0.025165       -0.124897   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986287     419.791378  \n",
       "2                 0.986276     426.281453  \n",
       "3                 0.986093     397.558125  \n",
       "4                 0.986044     209.449511  \n",
       "5                 0.984828     257.370240  \n",
       "6                 0.974170     260.817710  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10.sort_values(['rank_test_R2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>-0.091036</td>\n",
       "      <td>0.986287</td>\n",
       "      <td>419.791378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-0.013357</td>\n",
       "      <td>-0.091042</td>\n",
       "      <td>0.986276</td>\n",
       "      <td>426.281453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.091617</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>397.558125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-0.013584</td>\n",
       "      <td>-0.091974</td>\n",
       "      <td>0.986044</td>\n",
       "      <td>209.449511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>-0.014770</td>\n",
       "      <td>-0.095160</td>\n",
       "      <td>0.984828</td>\n",
       "      <td>257.370240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.025165</td>\n",
       "      <td>-0.124897</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>260.817710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling         100000  squared_loss       0.05   \n",
       "2                     invscaling         100000  squared_loss        0.1   \n",
       "3                     invscaling         100000  squared_loss       0.01   \n",
       "4                       constant         100000  squared_loss       0.01   \n",
       "5                       constant         100000  squared_loss       0.05   \n",
       "6                       constant         100000  squared_loss        0.1   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000483   \n",
       "2                     None               2               2       0.000491   \n",
       "3                     None               3               3       0.000495   \n",
       "4                     None               4               4       0.000407   \n",
       "5                     None               5               5       0.001060   \n",
       "6                     None               6               6       0.006079   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001480     0.000550       -0.013347       -0.091036   \n",
       "2                  0.001504     0.000558       -0.013357       -0.091042   \n",
       "3                  0.001434     0.000563       -0.013536       -0.091617   \n",
       "4                  0.001359     0.000468       -0.013584       -0.091974   \n",
       "5                  0.003164     0.001077       -0.014770       -0.095160   \n",
       "6                  0.015979     0.006186       -0.025165       -0.124897   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986287     419.791378  \n",
       "2                 0.986276     426.281453  \n",
       "3                 0.986093     397.558125  \n",
       "4                 0.986044     209.449511  \n",
       "5                 0.984828     257.370240  \n",
       "6                 0.974170     260.817710  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10.sort_values(['rank_test_-MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>rank_test_-MSE</th>\n",
       "      <th>rank_test_-MAE</th>\n",
       "      <th>std_test_-MSE</th>\n",
       "      <th>std_test_-MAE</th>\n",
       "      <th>std_test_R2</th>\n",
       "      <th>mean_test_-MSE</th>\n",
       "      <th>mean_test_-MAE</th>\n",
       "      <th>mean_test_R2</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_R2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>-0.091036</td>\n",
       "      <td>0.986287</td>\n",
       "      <td>419.791378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-0.013357</td>\n",
       "      <td>-0.091042</td>\n",
       "      <td>0.986276</td>\n",
       "      <td>426.281453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invscaling</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.091617</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>397.558125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-0.013584</td>\n",
       "      <td>-0.091974</td>\n",
       "      <td>0.986044</td>\n",
       "      <td>209.449511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>-0.014770</td>\n",
       "      <td>-0.095160</td>\n",
       "      <td>0.984828</td>\n",
       "      <td>257.370240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>constant</td>\n",
       "      <td>100000</td>\n",
       "      <td>squared_loss</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>-0.025165</td>\n",
       "      <td>-0.124897</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>260.817710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param_learning_rate param_max_iter    param_loss param_eta0  \\\n",
       "rank_test_R2                                                               \n",
       "1                     invscaling         100000  squared_loss       0.05   \n",
       "2                     invscaling         100000  squared_loss        0.1   \n",
       "3                     invscaling         100000  squared_loss       0.01   \n",
       "4                       constant         100000  squared_loss       0.01   \n",
       "5                       constant         100000  squared_loss       0.05   \n",
       "6                       constant         100000  squared_loss        0.1   \n",
       "\n",
       "             param_penalty  rank_test_-MSE  rank_test_-MAE  std_test_-MSE  \\\n",
       "rank_test_R2                                                                \n",
       "1                     None               1               1       0.000483   \n",
       "2                     None               2               2       0.000491   \n",
       "3                     None               3               3       0.000495   \n",
       "4                     None               4               4       0.000407   \n",
       "5                     None               5               5       0.001060   \n",
       "6                     None               6               6       0.006079   \n",
       "\n",
       "              std_test_-MAE  std_test_R2  mean_test_-MSE  mean_test_-MAE  \\\n",
       "rank_test_R2                                                               \n",
       "1                  0.001480     0.000550       -0.013347       -0.091036   \n",
       "2                  0.001504     0.000558       -0.013357       -0.091042   \n",
       "3                  0.001434     0.000563       -0.013536       -0.091617   \n",
       "4                  0.001359     0.000468       -0.013584       -0.091974   \n",
       "5                  0.003164     0.001077       -0.014770       -0.095160   \n",
       "6                  0.015979     0.006186       -0.025165       -0.124897   \n",
       "\n",
       "              mean_test_R2  mean_fit_time  \n",
       "rank_test_R2                               \n",
       "1                 0.986287     419.791378  \n",
       "2                 0.986276     426.281453  \n",
       "3                 0.986093     397.558125  \n",
       "4                 0.986044     209.449511  \n",
       "5                 0.984828     257.370240  \n",
       "6                 0.974170     260.817710  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10.sort_values(['rank_test_-MSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta0': 0.05,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 100000,\n",
       " 'penalty': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
